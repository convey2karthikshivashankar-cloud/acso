#!/usr/bin/env python3
"""
ACSO Human-in-the-Loop Workflow Integration Tests

Tests for validating human approval workflows, escalation procedures,
and human-AI collaboration in the ACSO system.
"""

import asyncio
import json
import pytest
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from unittest.mock import AsyncMock, MagicMock, patch

from src.shared.human_interface import (
    HumanApprovalInterface, ApprovalRequest, ApprovalResponse,
    EscalationRequest, EscalationResponse, HumanDecision
)
from src.shared.models import (
    AgentMessage, WorkflowStatus, ThreatAlert, IncidentReport,
    ServiceRequest, ApprovalStatus
)
from src.shared.communication import MessageBus
from src.shared.coordination import WorkflowCoordinator
from src.agents.supervisor_agent import SupervisorAgent
from config.settings import Settings


class HumanApprovalTestFramework:
    """Framework for testing human approval workflows."""
    
    def __init__(self):
        self.settings = Settings()
        self.human_interface = HumanApprovalInterface()
        self.message_bus = MessageBus()
        self.workflow_coordinator = WorkflowCoordinator()
        self.supervisor = SupervisorAgent()
        
        # Mock human operators
        self.mock_operators = {
            "security-analyst-001": {
                "role": "security_analyst",
                "clearance_level": "high",
                "specialties": ["threat_analysis", "incident_response"]
            },
            "operations-manager-001": {
                "role": "operations_manager", 
                "clearance_level": "critical",
                "specialties": ["service_management", "change_approval"]
            },
            "senior-analyst-001": {
                "role": "senior_analyst",
                "clearance_level": "critical",
                "specialties": ["complex_analysis", "escalation_handling"]
            }
        }
        
        # Test tracking
        self.approval_requests = []
        self.escalation_requests = []
        self.human_decisions = []
        
    async def setup_test_environment(self):
        """Set up the test environment."""
        await self.message_bus.initialize()
        await self.human_interface.initialize()
        await self.workflow_coordinator.initialize(self.message_bus)
        
        # Configure human interface with mock operators
        await self.human_interface.configure_operators(self.mock_operators)
        
        # Set up supervisor
        self.supervisor.message_bus = self.message_bus
        self.supervisor.human_interface = self.human_interface
        self.supervisor.workflow_coordinator = self.workflow_coordinator
        await self.supervisor.start()
        
    async def teardown_test_environment(self):
        """Clean up the test environment."""
        await self.supervisor.stop()
        await self.workflow_coordinator.shutdown()
        await self.human_interface.shutdown()
        await self.message_bus.shutdown()
        
    def create_approval_scenario(self, scenario_type: str) -> Dict[str, Any]:
        """Create different approval scenarios for testing."""
        scenarios = {
            "high_risk_containment": {
                "request_type": "incident_response",
                "description": "Isolate critical production server due to suspected compromise",
                "risk_level": "high",
                "business_impact": "critical",
                "estimated_downtime": 60,
                "affected_systems": ["prod-web-01", "prod-db-01"],
                "requires_approval": True,
                "approval_timeout": 300,  # 5 minutes
                "fallback_action": "escalate"
            },
            "emergency_patch": {
                "request_type": "service_request",
                "description": "Deploy emergency security patch to all production systems",
                "risk_level": "medium",
                "business_impact": "moderate",
                "estimated_downtime": 30,
                "affected_systems": ["all_production"],
                "requires_approval": True,
                "approval_timeout": 600,  # 10 minutes
                "fallback_action": "deny"
            },
            "data_access_request": {
                "request_type": "data_access",
                "description": "Access customer data for forensic analysis",
                "risk_level": "high",
                "privacy_impact": "high",
                "data_sensitivity": "pii",
                "requires_approval": True,
                "approval_timeout": 1800,  # 30 minutes
                "fallback_action": "deny"
            }
        }
        return scenarios.get(scenario_type, {})
        
    def create_escalation_scenario(self, scenario_type: str) -> Dict[str, Any]:
        """Create escalation scenarios for testing."""
        scenarios = {
            "uncertain_threat": {
                "escalation_type": "threat_analysis",
                "description": "Unusual network pattern detected - requires expert analysis",
                "confidence_level": 0.45,
                "potential_impact": "unknown",
                "complexity": "high",
                "requires_human_judgment": True,
                "escalation_timeout": 900  # 15 minutes
            },
            "policy_conflict": {
                "escalation_type": "policy_decision",
                "description": "Automated response conflicts with business policy",
                "conflict_type": "business_rule_violation",
                "automated_recommendation": "block_all_traffic",
                "policy_constraint": "maintain_business_continuity",
                "requires_human_judgment": True,
                "escalation_timeout": 600  # 10 minutes
            },
            "resource_contention": {
                "escalation_type": "resource_allocation",
                "description": "Multiple critical incidents competing for limited resources",
                "competing_incidents": ["incident-001", "incident-002"],
                "available_resources": ["analyst-team-1"],
                "requires_prioritization": True,
                "escalation_timeout": 300  # 5 minutes
            }
        }
        return scenarios.get(scenario_type, {})


class TestBasicApprovalWorkflows:
    """Test basic human approval workflows."""
    
    @pytest.fixture
    async def approval_framework(self):
        """Pytest fixture for approval test framework."""
        framework = HumanApprovalTestFramework()
        await framework.setup_test_environment()
        yield framework
        await framework.teardown_test_environment()
        
    @pytest.mark.asyncio
    async def test_simple_approval_workflow(self, approval_framework):
        """Test basic approval request and response workflow."""
        framework = approval_framework
        
        # Create approval scenario
        scenario = framework.create_approval_scenario("high_risk_containment")
        
        # Create approval request
        approval_request = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            approval_type=scenario["request_type"],
            description=scenario["description"],
            requester="incident-response-001",
            risk_level=scenario["risk_level"],
            business_impact=scenario["business_impact"],
            estimated_downtime=scenario["estimated_downtime"],
            affected_systems=scenario["affected_systems"],
            timeout_seconds=scenario["approval_timeout"],
            fallback_action=scenario["fallback_action"],
            required_clearance="high"
        )
        
        # Submit approval request
        await framework.human_interface.submit_approval_request(approval_request)
        
        # Verify request was queued
        pending_approvals = await framework.human_interface.get_pending_approvals()
        assert len(pending_approvals) == 1
        assert pending_approvals[0].request_id == approval_request.request_id
        
        # Simulate human approval
        approval_response = ApprovalResponse(
            request_id=approval_request.request_id,
            approver="security-analyst-001",
            decision="approved",
            reasoning="Threat confirmed - immediate containment required",
            conditions=["Monitor system performance", "Prepare rollback plan"],
            timestamp=datetime.utcnow()
        )
        
        # Submit approval response
        await framework.human_interface.submit_approval_response(approval_response)
        
        # Verify approval was processed
        approval_status = await framework.human_interface.get_approval_status(
            approval_request.request_id
        )
        assert approval_status.status == "approved"
        assert approval_status.approver == "security-analyst-001"
        assert len(approval_status.conditions) == 2
        
        # Verify workflow was notified
        workflow_messages = await framework.message_bus.get_messages_for_workflow(
            approval_request.workflow_id
        )
        approval_notification = None
        for msg in workflow_messages:
            if msg.message_type == "approval_decision":
                approval_notification = msg
                break
                
        assert approval_notification is not None
        assert approval_notification.payload["decision"] == "approved"
        
        print(f"‚úÖ Simple approval workflow completed: {approval_request.request_id}")
        
    @pytest.mark.asyncio
    async def test_approval_denial_workflow(self, approval_framework):
        """Test approval denial workflow."""
        framework = approval_framework
        
        # Create approval scenario
        scenario = framework.create_approval_scenario("data_access_request")
        
        approval_request = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            approval_type=scenario["request_type"],
            description=scenario["description"],
            requester="threat-hunter-001",
            risk_level=scenario["risk_level"],
            privacy_impact=scenario["privacy_impact"],
            data_sensitivity=scenario["data_sensitivity"],
            timeout_seconds=scenario["approval_timeout"],
            fallback_action=scenario["fallback_action"],
            required_clearance="critical"
        )
        
        await framework.human_interface.submit_approval_request(approval_request)
        
        # Simulate denial
        denial_response = ApprovalResponse(
            request_id=approval_request.request_id,
            approver="operations-manager-001",
            decision="denied",
            reasoning="Insufficient justification for PII access - alternative methods available",
            alternative_actions=["Use anonymized data", "Request legal review"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_approval_response(denial_response)
        
        # Verify denial was processed
        approval_status = await framework.human_interface.get_approval_status(
            approval_request.request_id
        )
        assert approval_status.status == "denied"
        assert approval_status.approver == "operations-manager-001"
        assert len(approval_status.alternative_actions) == 2
        
        print(f"‚úÖ Approval denial workflow completed: {approval_request.request_id}")
        
    @pytest.mark.asyncio
    async def test_approval_timeout_handling(self, approval_framework):
        """Test approval timeout and fallback actions."""
        framework = approval_framework
        
        # Create approval with short timeout
        approval_request = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            approval_type="incident_response",
            description="Test timeout scenario",
            requester="incident-response-001",
            risk_level="medium",
            timeout_seconds=2,  # Very short timeout for testing
            fallback_action="auto_approve",
            required_clearance="high"
        )
        
        await framework.human_interface.submit_approval_request(approval_request)
        
        # Wait for timeout
        await asyncio.sleep(3)
        
        # Verify timeout was handled with fallback action
        approval_status = await framework.human_interface.get_approval_status(
            approval_request.request_id
        )
        assert approval_status.status == "timeout_auto_approved"
        assert approval_status.fallback_applied is True
        
        print(f"‚úÖ Approval timeout handling completed: {approval_request.request_id}")


class TestEscalationWorkflows:
    """Test escalation workflows for complex scenarios."""
    
    @pytest.fixture
    async def escalation_framework(self):
        """Pytest fixture for escalation test framework."""
        framework = HumanApprovalTestFramework()
        await framework.setup_test_environment()
        yield framework
        await framework.teardown_test_environment()
        
    @pytest.mark.asyncio
    async def test_threat_analysis_escalation(self, escalation_framework):
        """Test escalation for uncertain threat analysis."""
        framework = escalation_framework
        
        # Create uncertain threat scenario
        scenario = framework.create_escalation_scenario("uncertain_threat")
        
        escalation_request = EscalationRequest(
            escalation_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            escalation_type=scenario["escalation_type"],
            description=scenario["description"],
            originating_agent="threat-hunter-001",
            confidence_level=scenario["confidence_level"],
            complexity=scenario["complexity"],
            potential_impact=scenario["potential_impact"],
            requires_expert_analysis=True,
            timeout_seconds=scenario["escalation_timeout"],
            required_expertise=["advanced_threat_analysis", "network_forensics"]
        )
        
        # Submit escalation
        await framework.human_interface.submit_escalation_request(escalation_request)
        
        # Verify escalation was queued
        pending_escalations = await framework.human_interface.get_pending_escalations()
        assert len(pending_escalations) == 1
        assert pending_escalations[0].escalation_id == escalation_request.escalation_id
        
        # Simulate expert analysis
        expert_analysis = {
            "threat_classification": "advanced_persistent_threat",
            "confidence_level": 0.87,
            "recommended_actions": [
                "deep_packet_inspection",
                "behavioral_analysis", 
                "threat_intelligence_correlation"
            ],
            "priority": "high",
            "estimated_investigation_time": 120  # 2 hours
        }
        
        escalation_response = EscalationResponse(
            escalation_id=escalation_request.escalation_id,
            analyst="senior-analyst-001",
            decision="investigate_further",
            analysis_results=expert_analysis,
            recommended_actions=expert_analysis["recommended_actions"],
            priority_adjustment="high",
            additional_resources_needed=["forensics_team"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_escalation_response(escalation_response)
        
        # Verify escalation was resolved
        escalation_status = await framework.human_interface.get_escalation_status(
            escalation_request.escalation_id
        )
        assert escalation_status.status == "resolved"
        assert escalation_status.analyst == "senior-analyst-001"
        assert escalation_status.decision == "investigate_further"
        
        print(f"‚úÖ Threat analysis escalation completed: {escalation_request.escalation_id}")
        
    @pytest.mark.asyncio
    async def test_policy_conflict_escalation(self, escalation_framework):
        """Test escalation for policy conflicts."""
        framework = escalation_framework
        
        scenario = framework.create_escalation_scenario("policy_conflict")
        
        escalation_request = EscalationRequest(
            escalation_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            escalation_type=scenario["escalation_type"],
            description=scenario["description"],
            originating_agent="incident-response-001",
            conflict_type=scenario["conflict_type"],
            automated_recommendation=scenario["automated_recommendation"],
            policy_constraint=scenario["policy_constraint"],
            requires_policy_decision=True,
            timeout_seconds=scenario["escalation_timeout"]
        )
        
        await framework.human_interface.submit_escalation_request(escalation_request)
        
        # Simulate policy decision
        policy_decision = {
            "decision": "modified_response",
            "modification": "selective_traffic_blocking",
            "justification": "Maintain critical business services while blocking malicious traffic",
            "policy_exception": True,
            "monitoring_requirements": ["continuous_traffic_analysis", "business_impact_monitoring"]
        }
        
        escalation_response = EscalationResponse(
            escalation_id=escalation_request.escalation_id,
            analyst="operations-manager-001",
            decision=policy_decision["decision"],
            policy_decision=policy_decision,
            business_justification=policy_decision["justification"],
            monitoring_requirements=policy_decision["monitoring_requirements"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_escalation_response(escalation_response)
        
        # Verify policy conflict was resolved
        escalation_status = await framework.human_interface.get_escalation_status(
            escalation_request.escalation_id
        )
        assert escalation_status.status == "resolved"
        assert escalation_status.decision == "modified_response"
        assert escalation_status.policy_exception_granted is True
        
        print(f"‚úÖ Policy conflict escalation completed: {escalation_request.escalation_id}")


class TestComplexApprovalScenarios:
    """Test complex approval scenarios with multiple stakeholders."""
    
    @pytest.fixture
    async def complex_framework(self):
        """Pytest fixture for complex approval scenarios."""
        framework = HumanApprovalTestFramework()
        await framework.setup_test_environment()
        yield framework
        await framework.teardown_test_environment()
        
    @pytest.mark.asyncio
    async def test_multi_stage_approval(self, complex_framework):
        """Test multi-stage approval process."""
        framework = complex_framework
        
        # Create complex scenario requiring multiple approvals
        workflow_id = str(uuid.uuid4())
        
        # Stage 1: Technical approval
        technical_approval = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=workflow_id,
            approval_type="technical_review",
            description="Emergency database maintenance requiring system downtime",
            requester="service-orchestration-001",
            risk_level="high",
            business_impact="critical",
            estimated_downtime=180,  # 3 hours
            affected_systems=["prod-db-cluster"],
            timeout_seconds=600,
            fallback_action="escalate",
            required_clearance="high",
            approval_stage=1,
            total_stages=3
        )
        
        await framework.human_interface.submit_approval_request(technical_approval)
        
        # Approve technical stage
        technical_response = ApprovalResponse(
            request_id=technical_approval.request_id,
            approver="security-analyst-001",
            decision="approved",
            reasoning="Technical approach is sound, security risks mitigated",
            conditions=["Backup verification required", "Rollback plan prepared"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_approval_response(technical_response)
        
        # Stage 2: Business approval
        business_approval = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=workflow_id,
            approval_type="business_impact_review",
            description="Business impact assessment for emergency maintenance",
            requester="service-orchestration-001",
            business_impact="critical",
            estimated_revenue_impact=50000,
            customer_impact="high",
            timeout_seconds=900,
            fallback_action="escalate",
            required_clearance="critical",
            approval_stage=2,
            total_stages=3,
            depends_on=[technical_approval.request_id]
        )
        
        await framework.human_interface.submit_approval_request(business_approval)
        
        # Approve business stage
        business_response = ApprovalResponse(
            request_id=business_approval.request_id,
            approver="operations-manager-001",
            decision="approved",
            reasoning="Critical security fix justifies business impact",
            conditions=["Customer notification required", "Support team on standby"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_approval_response(business_response)
        
        # Stage 3: Executive approval (for high-impact changes)
        executive_approval = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=workflow_id,
            approval_type="executive_approval",
            description="Executive sign-off for critical system maintenance",
            requester="operations-manager-001",
            business_impact="critical",
            estimated_revenue_impact=50000,
            timeout_seconds=1800,
            fallback_action="escalate",
            required_clearance="executive",
            approval_stage=3,
            total_stages=3,
            depends_on=[technical_approval.request_id, business_approval.request_id]
        )
        
        await framework.human_interface.submit_approval_request(executive_approval)
        
        # Simulate executive approval
        executive_response = ApprovalResponse(
            request_id=executive_approval.request_id,
            approver="senior-analyst-001",  # Acting as executive delegate
            decision="approved",
            reasoning="Security vulnerability requires immediate attention",
            conditions=["Post-maintenance review required", "Lessons learned documentation"],
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_approval_response(executive_response)
        
        # Verify all stages completed
        workflow_approval_status = await framework.human_interface.get_workflow_approval_status(
            workflow_id
        )
        assert workflow_approval_status.all_stages_approved is True
        assert len(workflow_approval_status.completed_stages) == 3
        assert workflow_approval_status.final_decision == "approved"
        
        print(f"‚úÖ Multi-stage approval completed: {workflow_id}")
        
    @pytest.mark.asyncio
    async def test_conditional_approval_workflow(self, complex_framework):
        """Test approval workflow with conditions and follow-up requirements."""
        framework = complex_framework
        
        # Create approval with conditions
        approval_request = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            workflow_id=str(uuid.uuid4()),
            approval_type="conditional_service_request",
            description="Deploy new security monitoring tool",
            requester="service-orchestration-001",
            risk_level="medium",
            business_impact="low",
            timeout_seconds=1200,
            fallback_action="deny",
            required_clearance="high"
        )
        
        await framework.human_interface.submit_approval_request(approval_request)
        
        # Approve with conditions
        conditional_response = ApprovalResponse(
            request_id=approval_request.request_id,
            approver="security-analyst-001",
            decision="approved_with_conditions",
            reasoning="Deployment approved pending security validation",
            conditions=[
                "Security scan must pass with zero critical findings",
                "Deployment must occur during maintenance window",
                "Rollback plan must be tested and verified"
            ],
            follow_up_required=True,
            follow_up_deadline=datetime.utcnow() + timedelta(hours=24),
            timestamp=datetime.utcnow()
        )
        
        await framework.human_interface.submit_approval_response(conditional_response)
        
        # Simulate condition fulfillment
        condition_updates = [
            {
                "condition": "Security scan must pass with zero critical findings",
                "status": "completed",
                "evidence": "Security scan report #12345 - 0 critical, 2 medium findings",
                "completed_by": "security-scanner-001",
                "timestamp": datetime.utcnow()
            },
            {
                "condition": "Deployment must occur during maintenance window", 
                "status": "scheduled",
                "evidence": "Deployment scheduled for 2024-01-20 02:00 UTC",
                "completed_by": "operations-manager-001",
                "timestamp": datetime.utcnow()
            },
            {
                "condition": "Rollback plan must be tested and verified",
                "status": "completed",
                "evidence": "Rollback test completed successfully - Test #67890",
                "completed_by": "service-orchestration-001",
                "timestamp": datetime.utcnow()
            }
        ]
        
        # Update condition statuses
        for condition_update in condition_updates:
            await framework.human_interface.update_approval_condition(
                approval_request.request_id,
                condition_update
            )
            
        # Verify all conditions met
        approval_status = await framework.human_interface.get_approval_status(
            approval_request.request_id
        )
        assert approval_status.all_conditions_met is True
        assert approval_status.ready_for_execution is True
        
        print(f"‚úÖ Conditional approval workflow completed: {approval_request.request_id}")


# Test utilities and runners
async def run_human_approval_tests():
    """Run all human approval workflow tests."""
    print("üë• Starting Human Approval Workflow Tests...")
    
    test_classes = [
        TestBasicApprovalWorkflows,
        TestEscalationWorkflows,
        TestComplexApprovalScenarios
    ]
    
    total_tests = 0
    passed_tests = 0
    
    for test_class in test_classes:
        print(f"\nüìã Running {test_class.__name__}...")
        
        # Create test instance
        test_instance = test_class()
        
        # Get test methods
        test_methods = [method for method in dir(test_instance) if method.startswith('test_')]
        
        for test_method_name in test_methods:
            total_tests += 1
            test_method = getattr(test_instance, test_method_name)
            
            try:
                print(f"   Running {test_method_name}...")
                
                # Create framework for each test
                framework = HumanApprovalTestFramework()
                await framework.setup_test_environment()
                
                try:
                    await test_method(framework)
                    passed_tests += 1
                    print(f"   ‚úÖ {test_method_name} PASSED")
                finally:
                    await framework.teardown_test_environment()
                    
            except Exception as e:
                print(f"   ‚ùå {test_method_name} FAILED: {e}")
                
    print(f"\nüìä Human Approval Test Results:")
    print(f"   Total Tests: {total_tests}")
    print(f"   Passed: {passed_tests}")
    print(f"   Failed: {total_tests - passed_tests}")
    print(f"   Success Rate: {(passed_tests/total_tests)*100:.1f}%")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    import sys
    
    # Run the human approval tests
    success = asyncio.run(run_human_approval_tests())
    sys.exit(0 if success else 1)